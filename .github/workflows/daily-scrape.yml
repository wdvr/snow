name: Daily Resort Scrape

on:
  schedule:
    # Run daily at 06:00 UTC (before ski day starts in most regions)
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Dry run (do not update database)'
        required: false
        type: boolean
        default: false

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    env:
      ENVIRONMENT: prod
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        working-directory: backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install beautifulsoup4 lxml

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2

      - name: Run scraper
        working-directory: backend
        run: |
          echo "Starting resort scraper..."
          python scripts/scrape_resorts.py \
            --output /tmp/resorts_scraped.json \
            --verbose 2>&1 | tee /tmp/scraper.log

          echo "Scraper completed. Resort count:"
          cat /tmp/resorts_scraped.json | python -c "import json,sys; d=json.load(sys.stdin); print(f\"Total: {len(d.get('resorts', d))}\")"
        timeout-minutes: 30

      - name: Get SNS Topic ARN
        id: get-sns
        run: |
          cd infrastructure
          pip install pulumi pulumi-aws -q
          pulumi login s3://snow-tracker-pulumi-state-us-west-2 --non-interactive 2>/dev/null || pulumi login --local
          pulumi stack select prod 2>/dev/null || true
          SNS_ARN=$(pulumi stack output resort_updates_topic_arn 2>/dev/null || echo "")
          echo "sns_arn=$SNS_ARN" >> $GITHUB_OUTPUT
        env:
          PULUMI_CONFIG_PASSPHRASE: ${{ secrets.PULUMI_CONFIG_PASSPHRASE }}

      - name: Populate resorts (dry run)
        if: github.event.inputs.dry_run == 'true'
        working-directory: backend
        env:
          RESORTS_TABLE: snow-tracker-resorts-prod
          PYTHONPATH: ${{ github.workspace }}/backend/src
        run: |
          python scripts/populate_resorts.py \
            --source /tmp/resorts_scraped.json \
            --dry-run \
            --detect-removed \
            --verbose

      - name: Populate resorts
        if: github.event.inputs.dry_run != 'true'
        working-directory: backend
        env:
          RESORTS_TABLE: snow-tracker-resorts-prod
          PYTHONPATH: ${{ github.workspace }}/backend/src
          SNS_TOPIC_ARN: ${{ steps.get-sns.outputs.sns_arn }}
        run: |
          python scripts/populate_resorts.py \
            --source /tmp/resorts_scraped.json \
            --detect-removed \
            --notify \
            --environment prod \
            --verbose

      - name: Upload scrape results
        uses: actions/upload-artifact@v4
        with:
          name: resort-scrape-${{ github.run_number }}
          path: |
            /tmp/resorts_scraped.json
            /tmp/scraper.log
          retention-days: 30

      - name: Trigger weather processor for new resorts
        if: github.event.inputs.dry_run != 'true'
        run: |
          aws lambda invoke \
            --function-name snow-tracker-weather-processor-prod \
            --invocation-type Event \
            --region us-west-2 \
            --payload '{}' \
            /tmp/weather-response.json
          echo "Weather processor triggered for new resorts"
